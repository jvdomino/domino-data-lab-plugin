---
description: Set up GenAI tracing for an agent or LLM application. Adds Domino SDK imports, @add_tracing decorators, and DominoRun context.
---

# /domino-trace-setup Command

Set up GenAI tracing for agent and LLM applications in Domino.

## Usage

```
/domino-trace-setup
```

## What This Command Does

1. **Checks environment requirements** (MLflow 3.2.0, Domino SDK)
2. **Creates tracing setup module** with decorators and context managers
3. **Generates example evaluators** for quality scoring
4. **Creates config.yaml** for agent configuration
5. **Provides example traced agent code**

## Output Files

### tracing_setup.py

```python
"""
Domino GenAI Tracing Setup
Generated by /domino-trace-setup

Requirements:
- mlflow==3.2.0
- dominodatalab[data,aisystems] @ git+https://github.com/dominodatalab/python-domino.git@master
"""

import mlflow
from domino.agents.tracing import add_tracing
from domino.agents.logging import DominoRun
import os

def setup_tracing(framework: str = "openai"):
    """
    Enable auto-tracing for LLM framework.

    Args:
        framework: One of 'openai', 'anthropic', 'langchain'
    """
    if framework == "openai":
        mlflow.openai.autolog()
    elif framework == "anthropic":
        mlflow.anthropic.autolog()
    elif framework == "langchain":
        mlflow.langchain.autolog()
    else:
        raise ValueError(f"Unknown framework: {framework}")

    print(f"Enabled {framework} auto-tracing")

def create_evaluator(metrics: list = None):
    """
    Create a basic evaluator function.

    Args:
        metrics: List of metrics to evaluate

    Returns:
        Evaluator function for @add_tracing
    """
    if metrics is None:
        metrics = ["quality_score", "response_length"]

    def evaluator(inputs, output):
        """
        Evaluate agent output.

        Args:
            inputs: Dict of function arguments
            output: Function return value

        Returns:
            Dict of metric names to values
        """
        scores = {}

        # Response length
        if isinstance(output, str):
            scores["response_length"] = len(output)
        elif isinstance(output, dict):
            scores["response_length"] = len(str(output))

        # Placeholder for quality score
        # Replace with actual evaluation logic
        scores["quality_score"] = 0.8

        return scores

    return evaluator

# Default evaluator
default_evaluator = create_evaluator()
```

### config.yaml

```yaml
# Agent Configuration
# Used with DominoRun(agent_config_path="config.yaml")

models:
  primary: gpt-4o-mini
  fallback: gpt-3.5-turbo
  judge: gpt-4o

agents:
  default:
    temperature: 0.7
    max_tokens: 1000

settings:
  retry_count: 3
  timeout_seconds: 30
```

### Example Agent (example_agent.py)

```python
"""
Example Traced Agent
Generated by /domino-trace-setup
"""

import mlflow
from domino.agents.tracing import add_tracing
from domino.agents.logging import DominoRun
from openai import OpenAI

# Setup
mlflow.openai.autolog()
client = OpenAI()

# Evaluator
def quality_evaluator(inputs, output):
    return {
        "response_length": len(output.get("response", "")),
        "confidence": output.get("confidence", 0),
    }

# Traced agent function
@add_tracing(name="my_agent", evaluator=quality_evaluator)
def my_agent(query: str) -> dict:
    """
    Example agent with tracing.

    Args:
        query: User question

    Returns:
        Dict with response and metadata
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": query}]
    )

    return {
        "response": response.choices[0].message.content,
        "confidence": 0.9,
        "model": "gpt-4o-mini"
    }

# Main execution
if __name__ == "__main__":
    # Define aggregated metrics
    aggregated_metrics = [
        ("response_length", "mean"),
        ("confidence", "mean"),
    ]

    # Run with tracing
    with DominoRun(
        run_name="example-run",
        agent_config_path="config.yaml",
        custom_summary_metrics=aggregated_metrics
    ) as run:
        # Test queries
        queries = [
            "What is machine learning?",
            "Explain neural networks",
            "How does gradient descent work?"
        ]

        for query in queries:
            result = my_agent(query)
            print(f"Q: {query}")
            print(f"A: {result['response'][:100]}...")
            print()

        print(f"Run ID: {run.run_id}")
```

## Environment Check

The command first verifies your environment:

```
üîç Checking environment requirements...

MLflow version: 3.2.0 ‚úÖ
Domino SDK: Installed with agents support ‚úÖ
OpenAI SDK: Installed ‚úÖ

Ready to set up GenAI tracing!
```

If requirements are missing:

```
‚ùå MLflow version 3.1.0 found, but 3.2.0 required

To fix, add to your Dockerfile:
RUN pip install mlflow==3.2.0
RUN pip install "dominodatalab[data,aisystems] @ git+https://github.com/dominodatalab/python-domino.git@master"
```

## Interactive Mode

When run interactively, the command prompts for:

1. **LLM framework** - OpenAI, Anthropic, or LangChain
2. **Agent name** - Name for your traced agent
3. **Include evaluator** - Whether to add quality scoring
4. **Aggregation metrics** - Which metrics to aggregate in runs

## Framework-Specific Setup

### OpenAI

```python
import mlflow
mlflow.openai.autolog()

from openai import OpenAI
client = OpenAI()
```

### Anthropic

```python
import mlflow
mlflow.anthropic.autolog()

from anthropic import Anthropic
client = Anthropic()
```

### LangChain

```python
import mlflow
mlflow.langchain.autolog()

from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o-mini")
```

## Viewing Traces

After running your traced code:

1. Go to **Experiments** in Domino
2. Find experiment: `tracing-{username}`
3. Select your run
4. Click **Traces** tab

The trace view shows:
- Span tree (agents, tools, messages)
- Token usage
- Latency
- Evaluator scores

## Related Commands

- `/domino-experiment-setup` - Set up traditional ML tracking
- `/domino-app-init` - Initialize web application

## Examples

```bash
# Basic setup
/domino-trace-setup

# With specific framework
/domino-trace-setup --framework anthropic

# With custom agent name
/domino-trace-setup --name incident-classifier
```
