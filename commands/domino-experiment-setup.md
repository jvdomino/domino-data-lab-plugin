---
description: Set up MLflow experiment tracking for traditional ML. Configures unique experiment names and auto-logging.
---

# /domino-experiment-setup Command

Set up MLflow experiment tracking for traditional machine learning projects in Domino.

## Usage

```
/domino-experiment-setup [experiment_name]
```

## What This Command Does

1. **Creates experiment setup code** with unique naming
2. **Configures auto-logging** for detected frameworks
3. **Adds Domino context** as MLflow tags
4. **Creates example training script** with best practices

## Output

### experiment_setup.py

```python
"""
Domino Experiment Tracking Setup
Generated by /domino-experiment-setup
"""

import mlflow
import os

def setup_experiment(base_name: str = "experiment"):
    """
    Set up a Domino-compatible MLflow experiment.

    IMPORTANT: Experiment names must be unique across the entire
    Domino deployment. This function appends username and project
    to ensure uniqueness.
    """
    username = os.environ.get('DOMINO_STARTING_USERNAME', 'unknown')
    project = os.environ.get('DOMINO_PROJECT_NAME', 'unknown')

    # Create unique experiment name
    experiment_name = f"{base_name}-{project}-{username}"

    mlflow.set_experiment(experiment_name)
    print(f"Experiment set: {experiment_name}")

    return experiment_name

def log_domino_context():
    """Log Domino environment information as tags."""
    mlflow.set_tags({
        "domino.user": os.environ.get('DOMINO_STARTING_USERNAME', 'unknown'),
        "domino.project": os.environ.get('DOMINO_PROJECT_NAME', 'unknown'),
        "domino.run_id": os.environ.get('DOMINO_RUN_ID', 'unknown'),
        "domino.hardware_tier": os.environ.get('DOMINO_HARDWARE_TIER_NAME', 'unknown'),
    })

# Auto-detect and enable framework logging
def setup_autolog():
    """Enable auto-logging for detected ML frameworks."""
    try:
        import sklearn
        mlflow.sklearn.autolog()
        print("Enabled sklearn auto-logging")
    except ImportError:
        pass

    try:
        import tensorflow
        mlflow.tensorflow.autolog()
        print("Enabled TensorFlow auto-logging")
    except ImportError:
        pass

    try:
        import torch
        mlflow.pytorch.autolog()
        print("Enabled PyTorch auto-logging")
    except ImportError:
        pass

    try:
        import xgboost
        mlflow.xgboost.autolog()
        print("Enabled XGBoost auto-logging")
    except ImportError:
        pass

    try:
        import lightgbm
        mlflow.lightgbm.autolog()
        print("Enabled LightGBM auto-logging")
    except ImportError:
        pass
```

### Example Usage in Training Script

```python
# train.py
from experiment_setup import setup_experiment, log_domino_context, setup_autolog
import mlflow
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Setup
experiment_name = setup_experiment("iris-classifier")
setup_autolog()

# Load data
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# Train with MLflow tracking
with mlflow.start_run(run_name="random-forest-v1"):
    # Log Domino context
    log_domino_context()

    # Custom parameters (in addition to auto-logged ones)
    mlflow.log_param("custom_param", "value")

    # Train model (auto-logged by mlflow.sklearn.autolog())
    model = RandomForestClassifier(n_estimators=100, max_depth=5)
    model.fit(X_train, y_train)

    # Custom metrics
    test_accuracy = model.score(X_test, y_test)
    mlflow.log_metric("test_accuracy", test_accuracy)

    print(f"Test accuracy: {test_accuracy:.4f}")
    print(f"Run ID: {mlflow.active_run().info.run_id}")
```

## Framework Detection

The command analyzes your project to detect ML frameworks:

```
ðŸ” Analyzing project dependencies...

Detected frameworks:
âœ… scikit-learn (from requirements.txt)
âœ… xgboost (from requirements.txt)
âŒ tensorflow (not found)
âŒ pytorch (not found)

Configuring auto-logging for: sklearn, xgboost
```

## Interactive Mode

When run without arguments:

1. **Prompts for experiment base name**
2. **Scans for ML frameworks**
3. **Asks about additional configuration**
4. **Creates setup files**

## Important Notes

### Experiment Name Uniqueness

> **CRITICAL**: Experiment names must be unique across the entire Domino deployment, not just your project.

The generated code automatically appends username and project to ensure uniqueness.

### Large Artifact Upload

For large models (LLMs, deep learning), the setup includes:

```python
# Enable multipart upload for large artifacts
os.environ['MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD'] = "true"
os.environ['MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE'] = "104857600"  # 100MB
```

## Related Commands

- `/domino-trace-setup` - Set up GenAI tracing (for LLM/agent projects)
- `/domino-app-init` - Initialize web application

## Examples

```bash
# Set up with default name
/domino-experiment-setup

# Set up with custom name
/domino-experiment-setup my-model-training

# Interactive mode with framework detection
/domino-experiment-setup --detect
```
